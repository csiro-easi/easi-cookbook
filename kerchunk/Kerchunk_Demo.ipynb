{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d1dc7d-d926-47f3-a379-d1293f439473",
   "metadata": {},
   "source": [
    "# Kerchunk Usage"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ad79a92-6078-44c8-80a8-5836e25cd10a",
   "metadata": {},
   "source": [
    "The kerchunk library allows you to create an index file system for a spatial dataset (consisting of one or more data files).\n",
    "With this index file system a user can easily build a lazy-loaded xarray Dataset view of the entire spatial dataset without the need for any external database or service.\n",
    "This xarray Dataset can then be chunked and distributed using dask as per usual.\n",
    "\n",
    "The index file system is a mapping between the high-level view of the entire dataset to each chunk of data stored across the many data files which compose the whole dataset.\n",
    "Only when data is accessed/computed, are the chunks read from their source data files.\n",
    "The index files store the byte ranges for each chunk of data which has been indexed, and only these bytes are read when needed (and in parallel when multiple chunks are needed).\n",
    "\n",
    "kerchunk can index different data file types like: NetCDF and Zarr.\n",
    "\n",
    "To index an entire product using kerchunk, two steps are required:\n",
    "1) Create a JSON index file for each data file.\n",
    "2) Merge all of the individual JSON index files into either:\n",
    "   a) One big JSON (human readable, much larger file size)\n",
    "   b) A Parquet directory (machine readable, more compact and faster to read)\n",
    "\n",
    "Once you have completed step 2, you can discard the output of step 1 if you desire.\n",
    "\n",
    "The one big JSON / Parquet directory can be stored anywhere; locally, next to the data files online, or somewhere else online. It doesn't matter where, they just need to be accessible by the end-user.\n",
    "NOTE: The index file/s reference the underlying data files using absolute URIs, not relative paths.\n",
    "\n",
    "To use the index file to create an xarray Dataset view of the data, a user requires permissions to access both the index file/s and the data files.\n",
    "\n",
    "This data access method uses the fsspec library to handle access to both the index file/s and the data files.\n",
    "Using this library a user can handle different file systems (local / AWS S3 / Google Cloud Store / HTTP / etc) and the required authentication associated.\n",
    "\n",
    "IMPORTANT: NetCDF and Zarr files can be internally chunked; this chunking is important and is reflected in the resulting kerchunk index file/s."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e64fbdd-3674-48b4-a9c6-10836962617c",
   "metadata": {},
   "source": [
    "fsspec\n",
    "\n",
    "The file access is done using the fsspec library. When accessing datasets we are pointing to two distinct things, the kerchunk index file/s and the data files.\n",
    "fsspec allows us to handle cases where the index and data files are stored in different places. Often they will both be in the same place (e.g. both on S3), but when they are not, it is supported.\n",
    "\n",
    "You will see this in the examples below where I am often creating and referencing the kerchunk index files locally, but the data files are in AWS S3.\n",
    "\n",
    "Examples:\n",
    "\n",
    "# 1) Local index files, Data files on S3\n",
    "\n",
    "fs = fsspec.filesystem(\n",
    "    protocol = \"reference\",\n",
    "    fo = \"/local/file/filename.ext\", # Index file path (a local file in this case)\n",
    "    remote_protocol = \"s3\", # This tells the system that the underlying data to which the index file points is found in AWS S3.\n",
    "    remote_options = {}, # This allows us to pass additional args related to the remote access location (like authentication).\n",
    "    skip_instance_cache = True\n",
    ")\n",
    "\n",
    "For AWS S3 access, you can also set env vars which will automatically get picked up and used, or you can create a boto session object and pass it into the fsspec.filesystem function.\n",
    "\n",
    "2) Index files and data files on S3\n",
    "\n",
    "fs = fsspec.filesystem(\n",
    "    protocol = \"reference\",\n",
    "    fo = \"s3://bucket/prefix/index.json\", # Index file is on S3\n",
    "    remote_protocol = \"s3\", # Data files are also on S3\n",
    "    remote_options={}\n",
    ")\n",
    "\n",
    "3) Local index files and data files on TERN Landscapes Data Store\n",
    "\n",
    "API_KEY = \"<your_secret_tern_api_key>\" # Remember not to keep keys around in code, this is just an example\n",
    "\n",
    "fs = fsspec.filesystem(\n",
    "    \"reference\",\n",
    "    fo = \"/local/file/index.parq\", # Index Parquet file is local\n",
    "    remote_protocol = \"https\", # Data files are accessed via authenticated HTTPS\n",
    "    remote_options = { \"headers\": { 'x-api-key' : API_KEY } },\n",
    "    retries = 10,\n",
    "    timeout = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe5929-6372-48b1-af84-86f1af49ac8b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba27a6a-3494-4c90-b449-9a7c3d99f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import ujson\n",
    "import xarray as xr\n",
    "\n",
    "from tqdm import tqdm # Optional: Progress bar for potentially long running operations\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a8eea-26d7-4701-97f8-51b0498aadd6",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8e6bd-e2a0-4dad-96f6-9dab4205f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_PREFIX = 's3://landscapes-easi-shared/misc/demo/himawari/lst'\n",
    "LOCAL_DIR = '/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906a5a7-3133-4702-972e-20d2a2f80937",
   "metadata": {},
   "source": [
    "## Single data file indexing and use"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea9c2660-3328-42a4-a4ed-9678b0543d11",
   "metadata": {},
   "source": [
    "It is possible to index just one data file and use it directly if desired.\n",
    "This is not a common use-case, but is shown here as a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82de62-ee37-46b1-8880-5ccdd85f5dc8",
   "metadata": {},
   "source": [
    "### Creating a single index JSON for a single data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c716f-7401-4c63-95ef-82eccd3296d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://fsspec.github.io/kerchunk/reference.html\n",
    "\n",
    "# Example: Data file is stored in AWS S3, index JSON will be created locally.\n",
    "\n",
    "data_file_url = S3_PREFIX + \"/2016/01/01/20160101000000_AHI_ANU_LSTv1.4.1_AusSubset.nc\"\n",
    "index_json_file_path = os.path.join(LOCAL_DIR, \"single_file_kerchunk.json\")\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\") # Can pass in additional args for authentication to S3 if required here.\n",
    "\n",
    "with fs.open(data_file_url) as inf:\n",
    "    # Generate index JSON\n",
    "    h5chunks = SingleHdf5ToZarr(inf, data_file_url, inline_threshold=100)\n",
    "    h5chunks.translate()\n",
    "    # Write index JSON to file\n",
    "    with open(index_json_file_path, \"wb\") as f:\n",
    "        f.write(ujson.dumps(h5chunks.translate()).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65b4bc-82d5-4b4d-9b50-ee6a04a97a71",
   "metadata": {},
   "source": [
    "### Accessing single indexed data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad480ba5-cc6a-43b4-9ce4-505d1bb3dd9d",
   "metadata": {},
   "source": [
    "## Data file collection indexing and use"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff05c839-10a4-4458-b6ee-dd1eac1b901e",
   "metadata": {},
   "source": [
    "A more common use case would be to index in many data files which together represent one data product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f008a54-db8a-4f10-af6a-aa5584595533",
   "metadata": {},
   "source": [
    "### Creating JSON index files for many data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce140e-c3be-46ff-9810-4cc39e476f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file system to read data files on AWS S3\n",
    "fs = fsspec.filesystem(\"s3\", skip_instance_cache=True)\n",
    "\n",
    "# Retrieve list of all data files at desired location\n",
    "s3_files = fs.glob(S3_PREFIX + \"/*/*/*/*.nc\")\n",
    "\n",
    "# Here we prepend the prefix 's3://' to each file path to make a full S3 URI\n",
    "s3_data_file_paths = sorted([\"s3://\" + f for f in s3_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46923bf0-903a-4208-bdf7-cfd89ae0ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of data files found: {len(s3_data_file_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88df203-d2c9-4558-ae93-cb0264b55c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_data_file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186064e4-256e-422d-af64-00d0f99c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the list of data file paths and create a JSON index file for each one\n",
    "\n",
    "temp_dir = os.path.join(LOCAL_DIR, \"demo_index_jsons\") # Local directory to write all the JSON index files to\n",
    "\n",
    "temp_dir_path = Path(temp_dir)\n",
    "if temp_dir_path.is_dir():\n",
    "    shutil.rmtree(temp_dir)\n",
    "temp_dir_path.mkdir()\n",
    "\n",
    "# Optional keyword arguments for fs.open (see fsspec doco for more information)\n",
    "so = {\n",
    "    \"mode\" : \"rb\",\n",
    "    \"default_fill_cache\" : False,\n",
    "    \"default_cache_type\" : \"first\"\n",
    "}\n",
    "\n",
    "def generate_json_reference(uri: str, temp_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate JSON index file for a data file and return the full file path to said JSON index file.\n",
    "    \"\"\"\n",
    "    with fs.open(uri, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, uri, inline_threshold=300)\n",
    "        fname = uri.split(\"/\")[-1].strip(\".nc\")\n",
    "        outf = os.path.join(temp_dir, f\"{fname}.json\")\n",
    "        with open(outf, \"wb\") as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode())\n",
    "        return outf\n",
    "\n",
    "# Iterate through the list of data files and generate JSON index files for each one. This loop could easily be parallelized or distributed using dask.\n",
    "output_files = []\n",
    "for data_file_path in tqdm(s3_data_file_paths):\n",
    "    outf = generate_json_reference(data_file_path, temp_dir)\n",
    "    output_files.append(outf)\n",
    "output_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712da06-2d09-4b11-acc8-108946fc0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TO_PRINT = 3\n",
    "for i in range(min(len(output_files), NUM_TO_PRINT)):\n",
    "    print(output_files[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2579a4-e0d2-43f0-923a-8743839f5819",
   "metadata": {},
   "source": [
    "#### Create combined JSON file to represent entire data product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac5c01-c754-40bf-a91e-a8db6b23d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine individual JSON index files into a single consolidated JSON file.\n",
    "\n",
    "# In this example, each NetCDF file contains different values on the time dimension, but they all share the same spatial dimensions.\n",
    "# NOTE: 'crs' is mentioned as a dimension due to how projection information can be stored in NetCDF files.\n",
    "\n",
    "combined_index_file_path = os.path.join(LOCAL_DIR, \"combined_kerchunk.json\")\n",
    "\n",
    "mzz = MultiZarrToZarr(\n",
    "    output_files,\n",
    "    concat_dims=[\"time\"],\n",
    "    identical_dims=[\"longitude\", \"latitude\", \"crs\"],\n",
    ")\n",
    "\n",
    "multi_kerchunk = mzz.translate()\n",
    "\n",
    "with open(combined_index_file_path, \"wb\") as f:\n",
    "    f.write(ujson.dumps(multi_kerchunk).encode())\n",
    "\n",
    "del multi_kerchunk\n",
    "del mzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b49205-fe4f-4099-8bbd-166679ffd5a0",
   "metadata": {},
   "source": [
    "#### Alternately, create Parquet index file system to represent entire data product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93276c-90fd-4cb1-b96c-fc9fb444fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path(os.path.join(LOCAL_DIR, \"demo_index_jsons\"))\n",
    "output_files = list(dir_path.rglob('*.json'))\n",
    "\n",
    "from fsspec.implementations.reference import LazyReferenceMapper\n",
    "\n",
    "parquet_directory = os.path.join(LOCAL_DIR, \"index.parq\")\n",
    "parquet_directory_path = Path(parquet_directory)\n",
    "\n",
    "if parquet_directory_path.is_dir():\n",
    "    shutil.rmtree(parquet_directory)\n",
    "parquet_directory_path.mkdir()\n",
    "\n",
    "fs = fsspec.filesystem(\"file\") # Am going to write this Parquet file system locally (could be remote if desired)\n",
    "\n",
    "out = LazyReferenceMapper.create(root=parquet_directory, fs=fs, record_size=1000)\n",
    "\n",
    "mzz = MultiZarrToZarr(\n",
    "    output_files,\n",
    "    remote_protocol = \"s3\",\n",
    "    concat_dims = [\"time\"],\n",
    "    identical_dims= [\"longitude\", \"latitude\", \"crs\"],\n",
    "    out=out,\n",
    ").translate()\n",
    "\n",
    "out.flush()\n",
    "\n",
    "del out\n",
    "del mzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557ae89-3d87-421d-82f5-fa9768019aea",
   "metadata": {},
   "source": [
    "### Accessing multi data file data product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99c9a2-6c72-4097-8591-ee69ceb1ae3e",
   "metadata": {},
   "source": [
    "#### Open using combined JSON index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cea5bd-19b6-4cf8-ad08-7a61198c8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\n",
    "    \"reference\",\n",
    "    fo = combined_index_file_path, # Index file is local in this case\n",
    "    remote_protocol = \"s3\", # Data files are stored on S3\n",
    "    remote_options= {} # Add authentication to S3 if required in here (see fsspec doco)\n",
    ")\n",
    "\n",
    "ds = xr.open_dataset(fs.get_mapper(\"\"), engine=\"zarr\", backend_kwargs=dict(consolidated=False), chunks={'time':12, 'latitude':5, 'longitude':5})\n",
    "\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0992a65-e2cb-4a30-b4f6-10add6d94941",
   "metadata": {},
   "source": [
    "#### Alternately, open using Parquet index file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f502f-306a-4800-a5cb-855370507200",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\n",
    "    \"reference\",\n",
    "    fo = parquet_directory, # Index file is local in this case\n",
    "    remote_protocol = \"s3\", # Data files are stored on S3\n",
    "    remote_options = {} # Add authentication to S3 if required in here (see fsspec doco)\n",
    ")\n",
    "\n",
    "ds = xr.open_dataset(fs.get_mapper(\"\"), engine=\"zarr\", backend_kwargs=dict(consolidated=False), chunks={'time':12, 'latitude':5, 'longitude':5})\n",
    "\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064c4da-5e33-49a2-b52d-6fb4c6afc279",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perform a temporal drill of one pixel through all available time\n",
    "\n",
    "ds_drill = ds['b1'].isel(latitude=slice(5,6), longitude=slice(10,11)).squeeze().compute()\n",
    "\n",
    "display(ds_drill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b0498-43b8-4062-8883-adf623fc1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perform a spatial read on one time slice\n",
    "\n",
    "ds_raster = ds['b1'].isel(time=slice(100,101)).squeeze().compute()\n",
    "\n",
    "display(ds_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d78a40-8430-47e4-95fb-8948ab09fb70",
   "metadata": {},
   "source": [
    "#### Open using a Parquet index file system with different chunking size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66860df3-e2bf-4a17-ba0a-50faccb5c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\n",
    "    \"reference\",\n",
    "    fo = parquet_directory, # Index file is local in this case\n",
    "    remote_protocol = \"s3\", # Data files are stored on S3\n",
    "    remote_options = {} # Add authentication to S3 if required in here (see fsspec doco)\n",
    ")\n",
    "\n",
    "# The default spatial chunking in this example is small and is great for minimising download size for temporal drills, but slower for spatial reads.\n",
    "# But you can easily increase the size of the chunks when you open the dataset which leads to more efficient (fewer, larger reads) data access.\n",
    "ds = xr.open_dataset(fs.get_mapper(\"\"), engine=\"zarr\", backend_kwargs=dict(consolidated=False), chunks={'time':12, 'latitude':10, 'longitude':10})\n",
    "\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5580ac-27c0-4c41-a55c-8926f157c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perform a temporal drill of one pixel through all available time\n",
    "\n",
    "ds_drill = ds['b1'].isel(latitude=slice(5,6), longitude=slice(10,11)).squeeze().compute()\n",
    "\n",
    "display(ds_drill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4ba12-ed7b-45b5-b971-bbbd1422c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perform a spatial read on one time slice\n",
    "\n",
    "ds_raster = ds['b1'].isel(time=slice(100,101)).squeeze().compute()\n",
    "\n",
    "display(ds_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53121bb5-3906-4e3e-91f0-9b9a72560b56",
   "metadata": {},
   "source": [
    "### HTTPS w/simple authentication (TERN Landscapes Data Services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75134020-ea19-4476-9e1e-1bb34f74860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to open an fsspec to TERN Landscapes Data Services\n",
    "\n",
    "API_KEY = \"<BYO_API_KEY>\" # Ingest and use your TERN API KEY here\n",
    "\n",
    "index_tern_path = \"s3://landscapes-easi-shared/misc/testing/himawari_tern_test/index_tern.parq\" # Example path to local or S3 based Parquet index file system\n",
    "\n",
    "fs = fsspec.filesystem(\n",
    "    \"reference\",\n",
    "    fo=index_tern_path,\n",
    "    remote_protocol=\"https\",\n",
    "    remote_options={ \"headers\": { 'x-api-key' : API_KEY } },\n",
    "    retries=10,\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d5051-a4a2-4099-b426-e5baed7924de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(fs.get_mapper(\"\"), engine=\"zarr\", backend_kwargs=dict(consolidated=False), chunks={'time':12, 'latitude':5, 'longitude':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59440ab2-2601-4042-ba4d-9541d245e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898beff-c2a8-4e52-92dd-941f6de3c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pixel = ds['b1'].isel(latitude=slice(0,1),longitude=slice(0,1)).squeeze()\n",
    "\n",
    "display(ds_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6f464-174b-4e1a-a60a-9c47c8bb02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds_pixel.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6979d5c-082a-4798-ab96-436b8e437698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kerchunk Env",
   "language": "python",
   "name": "kerchunk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
