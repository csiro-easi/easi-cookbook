## Usage: argo submit easi-dask-gateway.yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow

metadata:
  generateName: easi-dask-gateway-
  namespace: cenv-nbic-argo
  labels:
    team: R-19830-01
    owner: Matt

# TODO: Check
#  - onExit handler: kill all clusters in namespace (can not differentiate)
#  - mutex: one workflow at a time (because of the onExit handler)
#  - active deadline seconds: placeholder
#  - podGC: on workflow completion (success or error)
#  - dask workers: upload custom packages
#  - dask cluster address/id: write to an envvar for use by the notebooks

spec:
  serviceAccountName: cenv-nbic-team-sa-argo
  entrypoint: start

  podGC:
    strategy: OnPodSuccess  #OnPodCompletion
  onExit: exit-handler  # Shutdown all dask clusters
  synchronization:
    mutex:
      name:  dask_in_argo  # Allow only one "dask_in_argo" workflow at a time
  
  arguments:
    parameters:
      # USER INPUTS
      - name: wf_image
        # value: "444488357543.dkr.ecr.ap-southeast-2.amazonaws.com/easi-dask-noml:develop.latest"
        value: "444488357543.dkr.ecr.ap-southeast-2.amazonaws.com/easi-jupyter-noml-singleserver:develop.latest"
      # DASK-GATEWAY
      - name: dask_gateway_address
        value: "http://traefik-dask-gateway.easihub/services/dask-gateway"
      - name: dask_worker_image
        value: "444488357543.dkr.ecr.ap-southeast-2.amazonaws.com/easi-dask-noml:develop.latest"
      # ADD A CODE REPOSITORY (if not built in the wf_image)
      - name: package-repo
        value: "https://github.com/csiro-easi/easi-cookbook"
      - name: package-branch
        value: "argo-dask"
      - name: package-path
        value: "/opt/repo"
      - name: package-secret
        value: ""

  volumes:
    - name: argo-dask-service-account
      secret:
        secretName: cenv-nbic-team-jupyterhub-sa-api-token
        items:
          - key: jhub_api_token
            path: .jhub-api-token

  templates:
    - name: start
      steps:
      - - name: shutdown-all-clusters-start
          template: shutdown-all-clusters
      # - - name: generate-work
      #     template: generate-work

      # Either
      # - create dask cluster, pass to notebook via env var or config parameters
      # - - name: argo-creates-daskgateway
      #     template: argo-creates-daskgateway
      # Or
      # - set env vars to fake a jhub, then notebook creates cluster
      - - name: work-creates-daskgateway
          template: work-creates-daskgateway
  
      - - name: shutdown-all-clusters-end
          template: shutdown-all-clusters


    ##--------------------------------
    - name: generate-work
      script:
        image: "{{workflow.parameters.wf_image}}"
        imagePullPolicy: IfNotPresent
        command: [python]
        source: |
            print('Work created')


    ##--------------------------------
    # Argo creates dask-gateway
    - name: argo-creates-daskgateway
      steps:
      # - - name: get_aws_credentials
      #     template: get_aws_credentials
      - - name: init-dask-cluster
          template: init-dask-cluster
      # - - name: do-work
      #     template: do-work

    - name: init-dask-cluster
      script:
        image: "{{workflow.parameters.wf_image}}"
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: argo-dask-service-account
            mountPath: '/root'
        command: [python]
        source: |
          import sys
          from dask.distributed import Client
          from dask_gateway import Gateway
          from dask_gateway.auth import JupyterHubAuth
          from datacube.utils.aws import configure_s3_access
          from botocore.credentials import RefreshableCredentials

          import logging
          logging.basicConfig(level=logging.INFO)

          def create_cluster(
              gateway: Gateway,
              cluster_name: str = None,
              workers: tuple = (0,1),
              adapt = False,
              options = None,
          ):
              clusters = gateway.list_clusters()
              if cluster_name is not None and cluster_name in clusters:
                  # Connect to existing cluster
                  cluster = gateway.connect(cluster_name)   # This will block until the cluster enters the running state
                  logging.info(f'Connected to existing cluster: {cluster_name}')
              else:
                  # Start new cluster
                  cluster_name = gateway.submit(cluster_options=options)   # This is non-blocking, multiple clusters can be requested simultaneously
                  cluster = gateway.connect(cluster_name)
                  logging.info(f'Created new cluster: {cluster_name}')
              if adapt:
                  logging.info('Set cluster to adapt')
                  cluster.adapt(minimum = workers[0], maximum=workers[1])
              else:
                  logging.info('Set cluster to scale')
                  cluster.scale(workers[0])
              return cluster

          # Get api token from the mounted token
          with open('/root/.jhub-api-token','r') as f:
              api_token = f.read()
          address = "{{workflow.parameters.dask_gateway_address}}"
          
          auth = JupyterHubAuth(api_token=api_token)
          gateway = Gateway(address=address, auth=auth)
          
          # TESTING: Sanity check
          options = gateway.cluster_options()
          clusters = gateway.list_clusters()
          print(options)
          print(clusters)
          sys.exit(0)

          # Obtain AWS credentials from the serviceAccount assigned to this pod
          # Refresh if necessary and obtain the frozen version for use in the Dask Workers
          credentials = configure_s3_access(aws_unsigned=False, requester_pays=True)
          if isinstance(credentials, RefreshableCredentials):
              if credentials.refresh_needed():
                  credentials = configure_s3_access(aws_unsigned=False, requester_pays=True)
              logging.info(f'Credentials time remaining: {credentials._seconds_remaining()} seconds')
          frozen_creds = credentials.get_frozen_credentials()

          # Set Dask-gateway options
          # TODO: Get from input parameters
          options = gateway.cluster_options()
          adapt = False
          max_workers= 16
          options.node_selection = "worker8x"
          options.worker_cores = 8
          options.worker_memory = 30
          options.image = "{{workflow.parameters.dask_worker_image}}"
          options.aws_session_token = frozen_creds.token
          options.aws_access_key_id = frozen_creds.access_key
          options.aws_secret_access_key = frozen_creds.secret_key

          # Start dask cluster
          cluster = create_cluster(
              gateway=gateway,
              cluster_name = None,
              workers = (0,1),
              adapt = False,
              options = options,
          )

          # TODO: Pickle cluster for passing to the next Argo workers?
          # Not likely. Dask futures will/should die when the Argo worker finishes (and the python process completes)
          # Better to start a dask cluster (and re-use resources) for each Argo worker

          # TODO: Add the "do work" section here

          # TESTING: Shutdown the cluster
          client.close()
          cluster.shutdown()
          logging.info("Cluster shutdown")


    - name: do-work
      script:
        image: "{{workflow.parameters.wf_image}}"
        imagePullPolicy: IfNotPresent
        command: [python]
        source: |
          # Several strategies are described here for consideration, depending on your application
          # They have slightly different submission times and scheduler impacts
          # Which is most appropriate depends on the overall workload and the stability of some features in Dask (particularly strategy 1)
          # 
          # Strategy 1: Priority Map - processes the result task graphs in parallel and submit them
          #
          #   priority = list(reversed(range(len(results))))
          #
          #   client.wait_for_workers(n_workers=min_workers)
          #   logging.info("Waiting for workers")
          #
          #   client.scatter()
          #   results_future = client.scatter(results)
          #   futures = client.map(compute_result, results_future, priority)
          #   logging.info("Waiting for results...")
          #   results = client.gather(futures)
          #
          # Strategy 2: Compute each result one at a time. Will pause to process each task graph each time
          #
          #   results = list()
          #   for res in results:
          #       logging.info('Processing a tile...')
          #       a_result = res.compute(retries=3)
          #       results.append(a_result)
          #       logging.info('Completed a tile')
          #
          # Strategy 3: Submit one at a time, optimising one at a time. Will be scheduled as they become ready and executed when the scheduler can receive them.
          # It can be slow analysing the task graph for all Tiles together as Dask seeks out any repeated work to optimise out. This can take longer
          # than just reloading that section of data if there is one so in this case is not a useful optimisation.
          # fifo_timeout will assist in notifying the scheduler that these are different submissions and should be considered independently and done in submission order
          # this encourages dask to complete work (and reduce cluster memory pressure) though in practice there are a lot of available threads so everything stays busy
          #
          #   cluster.scale(min_workers)
          #   client.wait_for_workers(n_workers=min_workers)
          #   logging.info("Waiting for workers")
          #
          #   futures = list()
          #   for r in results:
          #       logging.info("Processing tile...")
          #       futures.append(
          #           client.compute(r, fifo_timeout='0ms',retries=3)    # Use client.compute() here so we get a Future back and can wait on it
          #       )
          #    logging.info("Waiting for results...")
          #    results = client.gather(futures)
          #
          #     logging.info("Results completed.")
          #     logging.info(results)

          import logging
          logging.basicConfig(level=logging.INFO)

          # Shutdown the cluster
          client.close()
          cluster.shutdown()
          logging.info("Cluster shutdown")


    ##--------------------------------
    - name: work-creates-daskgateway

      volumes:
      - name: git-sync
        emptyDir: {}

      initContainers:
        - name: init0
          image: alpine/git:latest
          imagePullPolicy: IfNotPresent
          mirrorVolumeMounts: true
          command: [/bin/sh, -c]
          args:
            - cd "{{workflow.parameters.package-path}}" &&
              git clone --depth 1 --branch "{{workflow.parameters.package-branch}}" "{{workflow.parameters.package-repo}}"

      script:
        image: "{{workflow.parameters.wf_image}}"
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: argo-dask-service-account
            mountPath: '/root'
          - name: git-sync
            mountPath: "{{workflow.parameters.package-path}}"
        # env:
        #   - name: DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE
        #     value: "{{workflow.parameters.dask_worker_image}}"
        #   - name: DASK_GATEWAY__CLUSTER__OPTIONS__EASI_USER_ALLOCATION
        #     value: "{{workflow.labels.team}}"
        #   - name: DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE_PULL_POLICY
        #     value: IfNotPresent
        #   - name: DASK_GATEWAY__ADDRESS    # or TRAEFIK_DASK_GATEWAY_SERVICE_HOST?
        #     value: "{{workflow.parameters.dask_gateway_address}}"
          
        # API_DASK_GATEWAY_PORT=tcp://172.20.212.192:8000
        # API_DASK_GATEWAY_PORT_8000_TCP=tcp://172.20.212.192:8000
        # API_DASK_GATEWAY_PORT_8000_TCP_ADDR=172.20.212.192
        # API_DASK_GATEWAY_PORT_8000_TCP_PORT=8000
        # API_DASK_GATEWAY_PORT_8000_TCP_PROTO=tcp
        # API_DASK_GATEWAY_SERVICE_HOST=172.20.212.192
        # API_DASK_GATEWAY_SERVICE_PORT=8000

        # DASK_GATEWAY__CLUSTER__OPTIONS__AWS_ACCESS_KEY_ID=
        # DASK_GATEWAY__CLUSTER__OPTIONS__AWS_SECRET_ACCESS_KEY=
        # DASK_GATEWAY__CLUSTER__OPTIONS__AWS_SESSION_TOKEN=
        # DASK_GATEWAY__CLUSTER__OPTIONS__EASI_USER_ALLOCATION='R-91296-1'
        # DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE=444488357543.dkr.ecr.ap-southeast-2.amazonaws.com/easi-dask-noml:develop.latest
        # DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE_PULL_POLICY=Always

        # See Jhub: /etc/dask/dask.yaml
        # DASK_GATEWAY__ADDRESS=http://traefik-dask-gateway.easihub/services/dask-gateway

        # TRAEFIK_DASK_GATEWAY_PORT=tcp://172.20.165.150:80
        # TRAEFIK_DASK_GATEWAY_PORT_80_TCP=tcp://172.20.165.150:80
        # TRAEFIK_DASK_GATEWAY_PORT_80_TCP_ADDR=172.20.165.150
        # TRAEFIK_DASK_GATEWAY_PORT_80_TCP_PORT=80
        # TRAEFIK_DASK_GATEWAY_PORT_80_TCP_PROTO=tcp
        # TRAEFIK_DASK_GATEWAY_SERVICE_HOST=172.20.165.150
        # TRAEFIK_DASK_GATEWAY_SERVICE_PORT=80
        # TRAEFIK_DASK_GATEWAY_SERVICE_PORT_WEB=80
        
        command: [python]
        source: |
          import os
          from dask_gateway.auth import JupyterHubAuth
          from datacube.utils.aws import configure_s3_access
          from botocore.credentials import RefreshableCredentials
          import subprocess
          from pathlib import Path

          import logging
          logging.basicConfig(level=logging.INFO)

          package_path = "{{workflow.parameters.package-path}}"
          package_repo = "{{workflow.parameters.package-repo}}"
          repo = Path(package_path) / package_repo.split('/')[-1]
          # sys.path.insert(1, str(repo))

          # Get api token from the mounted token
          with open('/root/.jhub-api-token','r') as f:
              api_token = f.read()

          # Obtain AWS credentials from the serviceAccount assigned to this pod
          # Refresh if necessary and obtain the frozen version for use in the Dask Workers
          credentials = configure_s3_access(aws_unsigned=False, requester_pays=True)
          if isinstance(credentials, RefreshableCredentials):
              if credentials.refresh_needed():
                  credentials = configure_s3_access(aws_unsigned=False, requester_pays=True)
              logging.info(f'Credentials time remaining: {credentials._seconds_remaining()} seconds')
          frozen_creds = credentials.get_frozen_credentials()

          env = {}
          env['DASK_GATEWAY__CLUSTER__OPTIONS__AWS_ACCESS_KEY_ID'] = frozen_creds.access_key
          env['DASK_GATEWAY__CLUSTER__OPTIONS__AWS_SECRET_ACCESS_KEY'] = frozen_creds.secret_key
          env['DASK_GATEWAY__CLUSTER__OPTIONS__AWS_SESSION_TOKEN'] = frozen_creds.token
          env['DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE'] = "{{workflow.parameters.dask_worker_image}}"
          env['DASK_GATEWAY__CLUSTER__OPTIONS__EASI_USER_ALLOCATION'] = "{{workflow.labels.team}}"
          env['DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE_PULL_POLICY'] = 'IfNotPresent'
          env['DASK_GATEWAY__ADDRESS'] = "{{workflow.parameters.dask_gateway_address}}"
          env['JUPYTERHUB_API_TOKEN'] = api_token


          notebook = 'argo-dask/simple-dask-gateway-example.ipynb'
          logging.info(f'Run the notebook please: {repo / notebook}')

          # jupyter nbconvert --ExecutePreprocessor.timeout=72000 \
          #     --ExecutePreprocessor.kernel_name=myenv --execute \
          #     --to notebook \
          #     f"{wf_params['package-path']}/{notebook}"
          cmd = [
              '/env/bin/jupyter', 'nbconvert',
              '--ExecutePreprocessor.timeout=72000',
              '--execute', '--to', 'notebook',
              '--output-dir', '/tmp',
              f'{repo / notebook}'
          ]
          cp = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=env)
          try:
              cp.check_returncode()
              logging.info('Notebook Success!')
              logging.info(cp.stdout.decode('utf-8'))
          except subprocess.CalledProcessError:
              logging.error(cp.stdout.decode('utf-8'))
              raise

          # TODO push the executed notebook to S3
          # - Get some outputs from the executed notebook?
          # - e.g., open as json and read the the cluster ID
          # - remove the script.env section above if its not needed

          


    ##--------------------------------
    # Exit handler templates
    # After the completion of the entrypoint template, the status of the
    # workflow is made available in the global variable {{workflow.status}}.
    # {{workflow.status}} will be one of: Succeeded, Failed, Error
    - name: exit-handler
      steps:
      - - name: shutdown-all-clusters
          template: shutdown-all-clusters
        - name: celebrate
          template: celebrate
          when: "{{workflow.status}} == Succeeded"
        - name: cry
          template: cry
          when: "{{workflow.status}} != Succeeded"

    - name: shutdown-all-clusters
      script:
        image: "{{workflow.parameters.wf_image}}"
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: argo-dask-service-account
            mountPath: '/root'
            readOnly: true
        command: [python]
        source: |
          from dask_gateway import Gateway
          from dask_gateway.auth import JupyterHubAuth

          import logging
          logging.basicConfig(level=logging.INFO)

          def shutdown_all_clusters(address, api_token):
              auth = JupyterHubAuth(api_token=api_token)
              gateway = Gateway(address=address,  auth=auth)
              clusters = gateway.list_clusters()
              logging.info(f"Shutting down these running clusters:\n {clusters}")
              for cluster in clusters:
                  gateway.connect(cluster.name).shutdown()

          with open('/root/.jhub-api-token', 'r') as f:
              api_token = f.read()
          address = '{{workflow.parameters.dask_gateway_address}}'

          logging.info("onExit Shutdown process")
          shutdown_all_clusters(address, api_token)

    # Option exists to send a notification somewhere
    - name: celebrate
      container:
        image: alpine:latest
        command: [sh, -c]
        args: ["echo hooray!"]

    - name: cry
      container:
        image: alpine:latest
        command: [sh, -c]
        args: ["echo boohoo!"]
